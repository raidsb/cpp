How lex and yacc work together?
Lex and Yacc are tools used for generating lexical analyzers (Lex) and parsers (Yacc/Bison). 
They work together by processing input files containing lexical and grammatical rules to 
produce C or C++ code that can recognize and parse the specified language.

Here's a simplified overview of how Lex and Yacc work together:

1- Lex:
Lex reads a set of regular expressions and corresponding actions from an input file (typically with a .l extension).
It generates a C/C++ source code file that defines a lexical analyzer (also called a lexer or scanner).
The generated lexer identifies tokens in the input text based on the specified regular expressions and 
executes the corresponding actions.

2- Yacc/Bison:
Yacc reads a set of grammar rules and associated actions from an input file (usually with a .y extension).
It generates a C/C++ source code file that contains a parser.
The generated parser recognizes the syntactic structure of the input based on the specified grammar rules 
and performs associated actions (e.g., constructing a parse tree).

3- Integration:
The output from Lex (lexer code) and Yacc/Bison (parser code) can be integrated into a single C/C++ program.
Typically, the lexer and parser are linked together to create a compiler or interpreter for a specific language.

4- Workflow:
Lex generates a lexical analyzer that identifies tokens in the input text (identifiers, keywords, operators, etc.).
Yacc generates a parser that processes the token stream generated by Lex according to the grammar rules, creating a structured representation of the input (e.g., an Abstract Syntax Tree).

5- Error Handling:
Both Lex and Yacc provide mechanisms for handling syntax errors and other issues in the input text.
By combining Lex for lexical analysis and Yacc/Bison for parsing, developers can create tools like 
compilers, interpreters, and syntax analyzers for specific programming or domain-specific languages. 
The generated code efficiently recognizes and processes the syntax of the input language.


In the typical workflow of lexical analysis and parsing, the lexer (generated by tools like Lex or Flex) 
is called first before the parser (created by tools like Yacc or Bison).

Here's a simplified overview of the process:

Lexer (or Scanner):
The lexer performs lexical analysis by breaking down the input text into smaller units called tokens 
(such as identifiers, keywords, literals, etc.).

It recognizes patterns in the input based on regular expressions specified in the lexer file.
The lexer generates tokens and passes them to the parser for further processing.

Parser:
The parser receives the tokens from the lexer and processes them based on a specified grammar 
(written in tools like Yacc or Bison).
It verifies whether the sequence of tokens received from the lexer matches the grammar rules.
The parser constructs a structured representation of the input (such as an Abstract Syntax Tree or 
another intermediate representation) according to the grammar rules.
So, the sequence is generally as follows: the lexer analyzes the input text, identifies tokens, 
and passes these tokens to the parser, which then analyzes the structure of these tokens based on 
the grammar rules.

This sequential process of lexical analysis followed by parsing is commonly used in the development
of compilers, interpreters, and other language processing tools.

--------------------------------------------------------------------------------------------------
Lex/Flex:
Lexical Analysis (Lex/Flex):
Lexical analysis involves converting sequences of characters into meaningful tokens. Here's a basic example using Lex/Flex:

Lex File Example (.l file):
lex
Copy code
%{
#include <stdio.h>
%}
// The section of regular expression to identify and create meaningful tokens
DIGIT   [0-9]
ID      [a-zA-Z_][a-zA-Z0-9_]*
%%
// sections of rules and actions associated with the token
{DIGIT}+    { printf("Integer: %s\n", yytext); }
{ID}        { printf("Identifier: %s\n", yytext); }
.           { printf("Invalid Character: %s\n", yytext); }
%%

int main() {
    yylex();
    return 0;
}
The code above defines patterns using regular expressions (DIGIT for integers, ID for identifiers) and specifies corresponding actions.
The %{ and %} sections include C code to be added in the generated lexer.
The %% section separates the rules and actions.
In the main() function, yylex() is called to start the lexical analysis.

-----------------------
Yacc:
Parsing (Yacc/Bison):
Parsing involves understanding the grammatical structure of the input language. Here's a simple example using 
Yacc/Bison:

Yacc File Example (.y file):
yacc
Copy code
%{
#include <stdio.h>
%}

%%
program : statement_list
        ;

statement_list : statement_list statement
               | statement
               ;

statement : ID '=' INT ';'
          | /* other statements */
          ;
%%

int main() {
    yyparse();
    return 0;
}
The code defines a grammar to recognize assignment statements (e.g., identifier = integer;).
The rules are written in BNF-like notation with corresponding actions.